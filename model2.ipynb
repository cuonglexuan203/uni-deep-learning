{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]           4,736\n",
      "       BatchNorm2d-2         [-1, 32, 224, 224]              64\n",
      "         MaxPool2d-3           [-1, 32, 44, 44]               0\n",
      "            Conv2d-4           [-1, 64, 44, 44]          51,264\n",
      "       BatchNorm2d-5           [-1, 64, 44, 44]             128\n",
      "         MaxPool2d-6             [-1, 64, 8, 8]               0\n",
      "            Conv2d-7            [-1, 128, 8, 8]         204,928\n",
      "            Conv2d-8            [-1, 128, 8, 8]         409,728\n",
      "            Conv2d-9            [-1, 256, 8, 8]         819,456\n",
      "           Conv2d-10            [-1, 256, 8, 8]         819,456\n",
      "           Conv2d-11             [-1, 64, 8, 8]         102,464\n",
      "           Conv2d-12            [-1, 256, 8, 8]       1,229,056\n",
      "           Conv2d-13            [-1, 256, 8, 8]       1,229,056\n",
      "           Conv2d-14            [-1, 512, 8, 8]       6,554,112\n",
      "           Conv2d-15            [-1, 512, 6, 6]       6,554,112\n",
      "AdaptiveAvgPool2d-16            [-1, 512, 3, 3]               0\n",
      "           Linear-17                   [-1, 32]         147,488\n",
      "           Linear-18                    [-1, 3]              99\n",
      "================================================================\n",
      "Total params: 18,126,147\n",
      "Trainable params: 18,126,147\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 27.98\n",
      "Params size (MB): 69.15\n",
      "Estimated Total Size (MB): 97.70\n",
      "----------------------------------------------------------------\n",
      "[1,    20] loss: 2.026\n",
      "Time: 2.3228707313537598\n",
      "[1,    40] loss: 1.059\n",
      "Time: 2.4657232761383057\n",
      "[1,    60] loss: 0.989\n",
      "Time: 2.0283188819885254\n",
      "[1,    80] loss: 0.939\n",
      "Time: 1.61000394821167\n",
      "[1,   100] loss: 0.851\n",
      "Time: 1.535212755203247\n",
      "[1,   120] loss: 0.868\n",
      "Time: 1.5857629776000977\n",
      "[1,   140] loss: 0.865\n",
      "Time: 1.8772337436676025\n",
      "[1,   160] loss: 0.854\n",
      "Time: 2.5456478595733643\n",
      "[1,   180] loss: 0.832\n",
      "Time: 2.9846091270446777\n",
      "[2,    20] loss: 0.815\n",
      "Time: 2.847043514251709\n",
      "[2,    40] loss: 0.711\n",
      "Time: 2.85571026802063\n",
      "[2,    60] loss: 0.736\n",
      "Time: 2.8228580951690674\n",
      "[2,    80] loss: 0.684\n",
      "Time: 2.8506922721862793\n",
      "[2,   100] loss: 0.734\n",
      "Time: 2.8187358379364014\n",
      "[2,   120] loss: 0.815\n",
      "Time: 2.831347942352295\n",
      "[2,   140] loss: 0.681\n",
      "Time: 2.8123233318328857\n",
      "[2,   160] loss: 0.639\n",
      "Time: 2.829280138015747\n",
      "[2,   180] loss: 0.684\n",
      "Time: 2.8249330520629883\n",
      "[3,    20] loss: 0.668\n",
      "Time: 2.8501150608062744\n",
      "[3,    40] loss: 0.624\n",
      "Time: 2.931830406188965\n",
      "[3,    60] loss: 0.556\n",
      "Time: 2.8080403804779053\n",
      "[3,    80] loss: 0.531\n",
      "Time: 2.8606107234954834\n",
      "[3,   100] loss: 0.565\n",
      "Time: 2.847179651260376\n",
      "[3,   120] loss: 0.635\n",
      "Time: 3.0971131324768066\n",
      "[3,   140] loss: 0.557\n",
      "Time: 3.4394888877868652\n",
      "[3,   160] loss: 0.589\n",
      "Time: 3.716257095336914\n",
      "[3,   180] loss: 0.588\n",
      "Time: 3.744119644165039\n",
      "[4,    20] loss: 0.499\n",
      "Time: 3.4507641792297363\n",
      "[4,    40] loss: 0.516\n",
      "Time: 3.5591630935668945\n",
      "[4,    60] loss: 0.486\n",
      "Time: 3.449892044067383\n",
      "[4,    80] loss: 0.630\n",
      "Time: 3.5000054836273193\n",
      "[4,   100] loss: 0.510\n",
      "Time: 3.2556259632110596\n",
      "[4,   120] loss: 0.857\n",
      "Time: 3.4449377059936523\n",
      "[4,   140] loss: 0.580\n",
      "Time: 3.3657171726226807\n",
      "[4,   160] loss: 0.491\n",
      "Time: 2.5646533966064453\n",
      "[4,   180] loss: 0.546\n",
      "Time: 2.330500364303589\n",
      "[5,    20] loss: 0.701\n",
      "Time: 2.346705913543701\n",
      "[5,    40] loss: 0.516\n",
      "Time: 2.3313934803009033\n",
      "[5,    60] loss: 0.563\n",
      "Time: 2.357154369354248\n",
      "[5,    80] loss: 0.558\n",
      "Time: 2.3398396968841553\n",
      "[5,   100] loss: 0.604\n",
      "Time: 2.362143039703369\n",
      "[5,   120] loss: 0.445\n",
      "Time: 2.3594133853912354\n",
      "[5,   140] loss: 0.469\n",
      "Time: 2.3772289752960205\n",
      "[5,   160] loss: 0.573\n",
      "Time: 2.366342067718506\n",
      "[5,   180] loss: 0.457\n",
      "Time: 2.365051746368408\n",
      "[6,    20] loss: 0.625\n",
      "Time: 2.340576171875\n",
      "[6,    40] loss: 0.516\n",
      "Time: 2.3532445430755615\n",
      "[6,    60] loss: 0.502\n",
      "Time: 2.33964204788208\n",
      "[6,    80] loss: 0.482\n",
      "Time: 2.375051498413086\n",
      "[6,   100] loss: 0.391\n",
      "Time: 2.3749682903289795\n",
      "[6,   120] loss: 0.492\n",
      "Time: 2.3587357997894287\n",
      "[6,   140] loss: 0.505\n",
      "Time: 2.3514528274536133\n",
      "[6,   160] loss: 0.581\n",
      "Time: 2.350050687789917\n",
      "[6,   180] loss: 0.455\n",
      "Time: 2.349447011947632\n",
      "[7,    20] loss: 0.473\n",
      "Time: 2.3698079586029053\n",
      "[7,    40] loss: 0.436\n",
      "Time: 2.360290288925171\n",
      "[7,    60] loss: 0.498\n",
      "Time: 2.3648297786712646\n",
      "[7,    80] loss: 0.416\n",
      "Time: 2.356168270111084\n",
      "[7,   100] loss: 0.551\n",
      "Time: 2.393974781036377\n",
      "[7,   120] loss: 0.417\n",
      "Time: 2.3947811126708984\n",
      "[7,   140] loss: 0.454\n",
      "Time: 2.31998872756958\n",
      "[7,   160] loss: 0.448\n",
      "Time: 2.350338935852051\n",
      "[7,   180] loss: 0.509\n",
      "Time: 2.3649284839630127\n",
      "[8,    20] loss: 0.434\n",
      "Time: 2.35361647605896\n",
      "[8,    40] loss: 0.420\n",
      "Time: 2.346513509750366\n",
      "[8,    60] loss: 0.522\n",
      "Time: 2.35502028465271\n",
      "[8,    80] loss: 0.482\n",
      "Time: 2.3597726821899414\n",
      "[8,   100] loss: 0.448\n",
      "Time: 2.425015449523926\n",
      "[8,   120] loss: 0.444\n",
      "Time: 2.3472208976745605\n",
      "[8,   140] loss: 0.460\n",
      "Time: 2.349992275238037\n",
      "[8,   160] loss: 0.389\n",
      "Time: 2.367608070373535\n",
      "[8,   180] loss: 0.436\n",
      "Time: 2.385774612426758\n",
      "[9,    20] loss: 0.527\n",
      "Time: 2.3496813774108887\n",
      "[9,    40] loss: 0.467\n",
      "Time: 2.3212404251098633\n",
      "[9,    60] loss: 0.447\n",
      "Time: 2.347700357437134\n",
      "[9,    80] loss: 0.413\n",
      "Time: 2.321444034576416\n",
      "[9,   100] loss: 0.400\n",
      "Time: 2.354485034942627\n",
      "[9,   120] loss: 0.443\n",
      "Time: 2.3485805988311768\n",
      "[9,   140] loss: 0.437\n",
      "Time: 2.3701088428497314\n",
      "[9,   160] loss: 0.408\n",
      "Time: 2.3284103870391846\n",
      "[9,   180] loss: 0.346\n",
      "Time: 2.3319709300994873\n",
      "[10,    20] loss: 0.452\n",
      "Time: 2.3441381454467773\n",
      "[10,    40] loss: 0.380\n",
      "Time: 2.3381824493408203\n",
      "[10,    60] loss: 0.485\n",
      "Time: 2.349494218826294\n",
      "[10,    80] loss: 0.434\n",
      "Time: 2.3481497764587402\n",
      "[10,   100] loss: 0.428\n",
      "Time: 2.347867965698242\n",
      "[10,   120] loss: 0.401\n",
      "Time: 2.3428585529327393\n",
      "[10,   140] loss: 0.440\n",
      "Time: 2.369889259338379\n",
      "[10,   160] loss: 0.438\n",
      "Time: 2.3560633659362793\n",
      "[10,   180] loss: 0.431\n",
      "Time: 2.326075792312622\n",
      "[11,    20] loss: 0.375\n",
      "Time: 2.3500070571899414\n",
      "[11,    40] loss: 0.405\n",
      "Time: 2.380345106124878\n",
      "[11,    60] loss: 0.397\n",
      "Time: 2.3510117530822754\n",
      "[11,    80] loss: 0.416\n",
      "Time: 2.3820838928222656\n",
      "[11,   100] loss: 0.449\n",
      "Time: 2.3697004318237305\n",
      "[11,   120] loss: 0.402\n",
      "Time: 2.3896899223327637\n",
      "[11,   140] loss: 0.353\n",
      "Time: 2.3746862411499023\n",
      "[11,   160] loss: 0.302\n",
      "Time: 2.342961072921753\n",
      "[11,   180] loss: 0.423\n",
      "Time: 2.333310604095459\n",
      "[12,    20] loss: 0.466\n",
      "Time: 2.3702054023742676\n",
      "[12,    40] loss: 0.453\n",
      "Time: 2.3524394035339355\n",
      "[12,    60] loss: 0.491\n",
      "Time: 2.382253885269165\n",
      "[12,    80] loss: 0.434\n",
      "Time: 2.332826852798462\n",
      "[12,   100] loss: 0.380\n",
      "Time: 2.3704586029052734\n",
      "[12,   120] loss: 0.271\n",
      "Time: 2.364549398422241\n",
      "[12,   140] loss: 0.341\n",
      "Time: 2.3590757846832275\n",
      "[12,   160] loss: 0.433\n",
      "Time: 2.3369762897491455\n",
      "[12,   180] loss: 0.374\n",
      "Time: 2.3638992309570312\n",
      "[13,    20] loss: 0.291\n",
      "Time: 2.350511074066162\n",
      "[13,    40] loss: 0.403\n",
      "Time: 2.3627803325653076\n",
      "[13,    60] loss: 0.339\n",
      "Time: 2.377009153366089\n",
      "[13,    80] loss: 0.290\n",
      "Time: 2.4128475189208984\n",
      "[13,   100] loss: 0.369\n",
      "Time: 2.4091312885284424\n",
      "[13,   120] loss: 0.344\n",
      "Time: 2.392472743988037\n",
      "[13,   140] loss: 0.404\n",
      "Time: 2.3918299674987793\n",
      "[13,   160] loss: 0.411\n",
      "Time: 2.3335464000701904\n",
      "[13,   180] loss: 0.512\n",
      "Time: 2.3933026790618896\n",
      "[14,    20] loss: 0.334\n",
      "Time: 2.34544038772583\n",
      "[14,    40] loss: 0.356\n",
      "Time: 2.3395321369171143\n",
      "[14,    60] loss: 0.346\n",
      "Time: 2.3346033096313477\n",
      "[14,    80] loss: 0.420\n",
      "Time: 2.3432207107543945\n",
      "[14,   100] loss: 0.327\n",
      "Time: 2.332193613052368\n",
      "[14,   120] loss: 0.447\n",
      "Time: 2.3354415893554688\n",
      "[14,   140] loss: 0.356\n",
      "Time: 2.334484577178955\n",
      "[14,   160] loss: 0.432\n",
      "Time: 2.3379836082458496\n",
      "[14,   180] loss: 0.351\n",
      "Time: 2.3475356101989746\n",
      "[15,    20] loss: 0.363\n",
      "Time: 2.3393795490264893\n",
      "[15,    40] loss: 0.462\n",
      "Time: 2.3269481658935547\n",
      "[15,    60] loss: 0.426\n",
      "Time: 2.3550820350646973\n",
      "[15,    80] loss: 0.356\n",
      "Time: 2.346040725708008\n",
      "[15,   100] loss: 0.346\n",
      "Time: 2.348073959350586\n",
      "[15,   120] loss: 0.376\n",
      "Time: 2.3348498344421387\n",
      "[15,   140] loss: 0.382\n",
      "Time: 2.332758903503418\n",
      "[15,   160] loss: 0.360\n",
      "Time: 2.33243465423584\n",
      "[15,   180] loss: 0.331\n",
      "Time: 2.370645523071289\n",
      "[16,    20] loss: 0.325\n",
      "Time: 2.372915267944336\n",
      "[16,    40] loss: 0.373\n",
      "Time: 2.329145908355713\n",
      "[16,    60] loss: 0.336\n",
      "Time: 2.3404741287231445\n",
      "[16,    80] loss: 0.369\n",
      "Time: 2.3565335273742676\n",
      "[16,   100] loss: 0.412\n",
      "Time: 2.3327298164367676\n",
      "[16,   120] loss: 0.289\n",
      "Time: 2.3955280780792236\n",
      "[16,   140] loss: 0.320\n",
      "Time: 2.4446094036102295\n",
      "[16,   160] loss: 0.397\n",
      "Time: 2.3879146575927734\n",
      "[16,   180] loss: 0.372\n",
      "Time: 2.3350329399108887\n",
      "[17,    20] loss: 0.326\n",
      "Time: 2.370513916015625\n",
      "[17,    40] loss: 0.373\n",
      "Time: 2.336498498916626\n",
      "[17,    60] loss: 0.312\n",
      "Time: 2.3728129863739014\n",
      "[17,    80] loss: 0.377\n",
      "Time: 2.3369789123535156\n",
      "[17,   100] loss: 0.279\n",
      "Time: 2.358874797821045\n",
      "[17,   120] loss: 0.293\n",
      "Time: 2.3583149909973145\n",
      "[17,   140] loss: 0.371\n",
      "Time: 2.368548631668091\n",
      "[17,   160] loss: 0.353\n",
      "Time: 2.3395261764526367\n",
      "[17,   180] loss: 0.378\n",
      "Time: 2.3496556282043457\n",
      "[18,    20] loss: 0.347\n",
      "Time: 2.3599820137023926\n",
      "[18,    40] loss: 0.290\n",
      "Time: 2.355879306793213\n",
      "[18,    60] loss: 0.332\n",
      "Time: 2.3640925884246826\n",
      "[18,    80] loss: 0.311\n",
      "Time: 2.3717041015625\n",
      "[18,   100] loss: 0.452\n",
      "Time: 2.3243935108184814\n",
      "[18,   120] loss: 0.293\n",
      "Time: 2.367628812789917\n",
      "[18,   140] loss: 0.371\n",
      "Time: 2.377328872680664\n",
      "[18,   160] loss: 0.341\n",
      "Time: 2.361069440841675\n",
      "[18,   180] loss: 0.330\n",
      "Time: 2.3544986248016357\n",
      "[19,    20] loss: 0.284\n",
      "Time: 2.3920562267303467\n",
      "[19,    40] loss: 0.327\n",
      "Time: 2.348252773284912\n",
      "[19,    60] loss: 0.357\n",
      "Time: 2.3372995853424072\n",
      "[19,    80] loss: 0.332\n",
      "Time: 2.3840184211730957\n",
      "[19,   100] loss: 0.292\n",
      "Time: 2.3397889137268066\n",
      "[19,   120] loss: 0.278\n",
      "Time: 2.3378169536590576\n",
      "[19,   140] loss: 0.313\n",
      "Time: 2.357598066329956\n",
      "[19,   160] loss: 0.391\n",
      "Time: 2.324726104736328\n",
      "[19,   180] loss: 0.360\n",
      "Time: 2.3840391635894775\n",
      "[20,    20] loss: 0.378\n",
      "Time: 2.3393702507019043\n",
      "[20,    40] loss: 0.283\n",
      "Time: 2.3541464805603027\n",
      "[20,    60] loss: 0.305\n",
      "Time: 2.335759401321411\n",
      "[20,    80] loss: 0.266\n",
      "Time: 2.339470863342285\n",
      "[20,   100] loss: 0.253\n",
      "Time: 2.360049247741699\n",
      "[20,   120] loss: 0.272\n",
      "Time: 2.344855308532715\n",
      "[20,   140] loss: 0.384\n",
      "Time: 2.340008020401001\n",
      "[20,   160] loss: 0.279\n",
      "Time: 2.3327689170837402\n",
      "[20,   180] loss: 0.242\n",
      "Time: 2.3353569507598877\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "def train():\n",
    "    # TRANSFORMATION AUGMENTATION\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        # Turn the image into a torch.Tensor\n",
    "        transforms.ToTensor(),  # converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "        # ConvFiltersTransform(axis=(1, 2)),\n",
    "        # transforms.Resize(size=(224, 224))\n",
    "    ])\n",
    "\n",
    "    # DATA SETS\n",
    "    train_data = datasets.ImageFolder(root='./dataset/train', transform=data_transform)\n",
    "    test_data = datasets.ImageFolder(root='./dataset/test', transform=data_transform)\n",
    "\n",
    "    # DATA LOADER\n",
    "    train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "    # print(train_data[0][0].shape)  # C, H, W\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=3)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            # Convolutional blocks\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            # self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "            self.conv3_1 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
    "            self.conv3_2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "            # Left side\n",
    "            self.conv4 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n",
    "            self.conv5 = nn.Conv2d(128, 256, kernel_size=5, padding=2)\n",
    "            self.conv6 = nn.Conv2d(128, 256, kernel_size=5, padding=2)\n",
    "            # Right side\n",
    "            self.conv7 = nn.Conv2d(192, 256, kernel_size=5, padding=2)\n",
    "            self.conv8 = nn.Conv2d(192, 256, kernel_size=5, padding=2)\n",
    "\n",
    "            #\n",
    "            self.conv9 = nn.Conv2d(512, 512, kernel_size=5, stride=1, padding=2)\n",
    "            self.conv10 = nn.Conv2d(512, 512, kernel_size=5, padding=1)\n",
    "\n",
    "            # Pooling layers\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=5, padding=0)\n",
    "            self.maxpool2 = nn.MaxPool2d(kernel_size=5, stride=5, padding=0)\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((3, 3))\n",
    "\n",
    "            # Fully connected layers (for now, we'll keep this as a placeholder)\n",
    "            self.fc1 = nn.Linear(512 * 3 * 3, 32)  # Adjust this later dynamically\n",
    "            self.fc6 = nn.Linear(32, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "            # Pooling\n",
    "            x = self.maxpool1(x)\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            x = self.maxpool2(x)\n",
    "            # Left side\n",
    "            x_1 = F.relu(self.conv3_1(x))\n",
    "            x_L = F.relu(self.conv4(x_1))\n",
    "            x_L_1 = F.relu(self.conv5(x_L))\n",
    "            x_L_2 = F.relu(self.conv6(x_L))\n",
    "            x_L = F.relu(torch.add(x_L_1, x_L_2))\n",
    "            # Right side\n",
    "            x_2 = F.relu(self.conv3_2(x))\n",
    "            x_R = torch.cat((x_1, x_2), 1)\n",
    "            x_R_1 = F.relu(self.conv7(x_R))\n",
    "            x_R_2 = F.relu(self.conv8(x_R))\n",
    "            x_R = F.relu(torch.add(x_R_1, x_R_2))\n",
    "            #\n",
    "            x = torch.cat((x_L, x_R), 1)\n",
    "            x = F.relu(self.conv9(x))\n",
    "            x = F.relu(self.conv10(x))\n",
    "            x = F.relu(self.avgpool(x))\n",
    "            #\n",
    "            \n",
    "            x = x.view(x.size(0), -1)  # Flatten\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc6(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on: {device}\")\n",
    "\n",
    "    net.to(device)\n",
    "    summary(net, input_size=(3, 224, 224))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        time_start = time.time()\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "                print('Time:', time.time() - time_start)\n",
    "                time_start = time.time()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
