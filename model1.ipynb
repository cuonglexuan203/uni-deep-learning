{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]           4,736\n",
      "       BatchNorm2d-2         [-1, 32, 224, 224]              64\n",
      "         MaxPool2d-3           [-1, 32, 44, 44]               0\n",
      "            Conv2d-4           [-1, 64, 44, 44]          18,496\n",
      "            Conv2d-5           [-1, 64, 44, 44]          36,928\n",
      "            Conv2d-6          [-1, 128, 44, 44]          73,856\n",
      "            Conv2d-7          [-1, 128, 44, 44]         147,584\n",
      "            Conv2d-8          [-1, 256, 44, 44]         295,168\n",
      "            Conv2d-9          [-1, 256, 44, 44]         295,168\n",
      "           Conv2d-10           [-1, 64, 44, 44]          36,928\n",
      "           Conv2d-11          [-1, 256, 44, 44]         442,624\n",
      "           Conv2d-12          [-1, 256, 44, 44]         442,624\n",
      "        AvgPool2d-13          [-1, 512, 11, 11]               0\n",
      "           Conv2d-14          [-1, 920, 11, 11]       4,240,280\n",
      "AdaptiveAvgPool2d-15            [-1, 920, 3, 3]               0\n",
      "           Linear-16                  [-1, 920]       7,618,520\n",
      "           Linear-17                    [-1, 3]           2,763\n",
      "================================================================\n",
      "Total params: 13,655,739\n",
      "Trainable params: 13,655,739\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 48.11\n",
      "Params size (MB): 52.09\n",
      "Estimated Total Size (MB): 100.77\n",
      "----------------------------------------------------------------\n",
      "[1,    20] loss: 1.160\n",
      "Time: 2.8151400089263916\n",
      "[1,    40] loss: 1.087\n",
      "Time: 2.6727147102355957\n",
      "[1,    60] loss: 1.052\n",
      "Time: 2.669520378112793\n",
      "[1,    80] loss: 0.944\n",
      "Time: 2.67912220954895\n",
      "[1,   100] loss: 0.888\n",
      "Time: 2.6705026626586914\n",
      "[1,   120] loss: 0.938\n",
      "Time: 2.6941258907318115\n",
      "[1,   140] loss: 0.878\n",
      "Time: 2.65177845954895\n",
      "[1,   160] loss: 0.846\n",
      "Time: 2.695159673690796\n",
      "[1,   180] loss: 0.801\n",
      "Time: 2.6528708934783936\n",
      "[2,    20] loss: 0.649\n",
      "Time: 2.6585445404052734\n",
      "[2,    40] loss: 0.623\n",
      "Time: 2.6790030002593994\n",
      "[2,    60] loss: 0.729\n",
      "Time: 2.6790354251861572\n",
      "[2,    80] loss: 0.720\n",
      "Time: 2.670109510421753\n",
      "[2,   100] loss: 0.618\n",
      "Time: 2.6603341102600098\n",
      "[2,   120] loss: 0.610\n",
      "Time: 2.678180456161499\n",
      "[2,   140] loss: 0.562\n",
      "Time: 2.6847078800201416\n",
      "[2,   160] loss: 0.630\n",
      "Time: 2.672759771347046\n",
      "[2,   180] loss: 0.577\n",
      "Time: 2.668128490447998\n",
      "[3,    20] loss: 0.757\n",
      "Time: 2.687784433364868\n",
      "[3,    40] loss: 0.662\n",
      "Time: 2.6851394176483154\n",
      "[3,    60] loss: 0.553\n",
      "Time: 2.683529853820801\n",
      "[3,    80] loss: 0.584\n",
      "Time: 2.6561150550842285\n",
      "[3,   100] loss: 0.609\n",
      "Time: 2.6757915019989014\n",
      "[3,   120] loss: 0.602\n",
      "Time: 2.7241127490997314\n",
      "[3,   140] loss: 0.631\n",
      "Time: 2.708566904067993\n",
      "[3,   160] loss: 0.646\n",
      "Time: 2.7427291870117188\n",
      "[3,   180] loss: 0.565\n",
      "Time: 2.75394344329834\n",
      "[4,    20] loss: 0.516\n",
      "Time: 2.7616400718688965\n",
      "[4,    40] loss: 0.630\n",
      "Time: 2.7054240703582764\n",
      "[4,    60] loss: 0.548\n",
      "Time: 2.704846143722534\n",
      "[4,    80] loss: 0.614\n",
      "Time: 2.7303216457366943\n",
      "[4,   100] loss: 0.594\n",
      "Time: 2.6591169834136963\n",
      "[4,   120] loss: 0.579\n",
      "Time: 2.670346975326538\n",
      "[4,   140] loss: 0.481\n",
      "Time: 2.710026502609253\n",
      "[4,   160] loss: 0.628\n",
      "Time: 2.6814162731170654\n",
      "[4,   180] loss: 0.456\n",
      "Time: 2.673370838165283\n",
      "[5,    20] loss: 0.657\n",
      "Time: 2.7014851570129395\n",
      "[5,    40] loss: 0.524\n",
      "Time: 2.677877902984619\n",
      "[5,    60] loss: 0.547\n",
      "Time: 2.684932231903076\n",
      "[5,    80] loss: 0.559\n",
      "Time: 2.663400411605835\n",
      "[5,   100] loss: 0.510\n",
      "Time: 2.672437906265259\n",
      "[5,   120] loss: 0.488\n",
      "Time: 2.6582541465759277\n",
      "[5,   140] loss: 0.488\n",
      "Time: 2.6431546211242676\n",
      "[5,   160] loss: 0.509\n",
      "Time: 2.6538922786712646\n",
      "[5,   180] loss: 0.514\n",
      "Time: 2.7003061771392822\n",
      "[6,    20] loss: 0.521\n",
      "Time: 2.6933224201202393\n",
      "[6,    40] loss: 0.561\n",
      "Time: 2.65492582321167\n",
      "[6,    60] loss: 0.488\n",
      "Time: 2.660078287124634\n",
      "[6,    80] loss: 0.650\n",
      "Time: 2.679865837097168\n",
      "[6,   100] loss: 0.515\n",
      "Time: 2.6782329082489014\n",
      "[6,   120] loss: 0.490\n",
      "Time: 2.6590640544891357\n",
      "[6,   140] loss: 0.460\n",
      "Time: 2.6495063304901123\n",
      "[6,   160] loss: 0.420\n",
      "Time: 2.6585168838500977\n",
      "[6,   180] loss: 0.543\n",
      "Time: 2.6825881004333496\n",
      "[7,    20] loss: 0.435\n",
      "Time: 2.6552376747131348\n",
      "[7,    40] loss: 0.582\n",
      "Time: 2.6965110301971436\n",
      "[7,    60] loss: 0.487\n",
      "Time: 2.6592395305633545\n",
      "[7,    80] loss: 0.416\n",
      "Time: 2.6500275135040283\n",
      "[7,   100] loss: 0.480\n",
      "Time: 2.7064192295074463\n",
      "[7,   120] loss: 0.535\n",
      "Time: 2.6933369636535645\n",
      "[7,   140] loss: 0.418\n",
      "Time: 2.6570003032684326\n",
      "[7,   160] loss: 0.463\n",
      "Time: 2.6450397968292236\n",
      "[7,   180] loss: 0.460\n",
      "Time: 2.662632465362549\n",
      "[8,    20] loss: 0.422\n",
      "Time: 2.669584274291992\n",
      "[8,    40] loss: 0.396\n",
      "Time: 2.668630599975586\n",
      "[8,    60] loss: 0.382\n",
      "Time: 2.67362642288208\n",
      "[8,    80] loss: 0.361\n",
      "Time: 2.67615008354187\n",
      "[8,   100] loss: 0.470\n",
      "Time: 2.705850601196289\n",
      "[8,   120] loss: 0.430\n",
      "Time: 2.701136589050293\n",
      "[8,   140] loss: 0.387\n",
      "Time: 2.718106985092163\n",
      "[8,   160] loss: 0.323\n",
      "Time: 2.7347285747528076\n",
      "[8,   180] loss: 0.419\n",
      "Time: 2.693588972091675\n",
      "[9,    20] loss: 0.327\n",
      "Time: 2.68361496925354\n",
      "[9,    40] loss: 0.402\n",
      "Time: 2.7139923572540283\n",
      "[9,    60] loss: 0.434\n",
      "Time: 2.7395334243774414\n",
      "[9,    80] loss: 0.461\n",
      "Time: 2.667625904083252\n",
      "[9,   100] loss: 0.418\n",
      "Time: 2.693049430847168\n",
      "[9,   120] loss: 0.485\n",
      "Time: 2.66697096824646\n",
      "[9,   140] loss: 0.404\n",
      "Time: 2.673175811767578\n",
      "[9,   160] loss: 0.401\n",
      "Time: 2.6670382022857666\n",
      "[9,   180] loss: 0.322\n",
      "Time: 2.664867639541626\n",
      "[10,    20] loss: 0.419\n",
      "Time: 2.678758382797241\n",
      "[10,    40] loss: 0.463\n",
      "Time: 2.656608819961548\n",
      "[10,    60] loss: 0.382\n",
      "Time: 2.655672073364258\n",
      "[10,    80] loss: 0.457\n",
      "Time: 2.6586554050445557\n",
      "[10,   100] loss: 0.442\n",
      "Time: 2.6885409355163574\n",
      "[10,   120] loss: 0.444\n",
      "Time: 2.680177927017212\n",
      "[10,   140] loss: 0.334\n",
      "Time: 2.668416738510132\n",
      "[10,   160] loss: 0.510\n",
      "Time: 2.690223217010498\n",
      "[10,   180] loss: 0.399\n",
      "Time: 2.6875839233398438\n",
      "[11,    20] loss: 0.395\n",
      "Time: 2.6633188724517822\n",
      "[11,    40] loss: 0.402\n",
      "Time: 2.6867916584014893\n",
      "[11,    60] loss: 0.390\n",
      "Time: 2.693704128265381\n",
      "[11,    80] loss: 0.297\n",
      "Time: 2.6690618991851807\n",
      "[11,   100] loss: 0.377\n",
      "Time: 2.700734853744507\n",
      "[11,   120] loss: 0.401\n",
      "Time: 2.77822208404541\n",
      "[11,   140] loss: 0.347\n",
      "Time: 2.811933755874634\n",
      "[11,   160] loss: 0.363\n",
      "Time: 2.911498546600342\n",
      "[11,   180] loss: 0.276\n",
      "Time: 2.708176374435425\n",
      "[12,    20] loss: 0.300\n",
      "Time: 2.6854536533355713\n",
      "[12,    40] loss: 0.394\n",
      "Time: 2.6637916564941406\n",
      "[12,    60] loss: 0.387\n",
      "Time: 2.650561571121216\n",
      "[12,    80] loss: 0.303\n",
      "Time: 2.673086166381836\n",
      "[12,   100] loss: 0.368\n",
      "Time: 2.9579532146453857\n",
      "[12,   120] loss: 0.406\n",
      "Time: 3.442626714706421\n",
      "[12,   140] loss: 0.343\n",
      "Time: 3.4136223793029785\n",
      "[12,   160] loss: 0.341\n",
      "Time: 3.448897123336792\n",
      "[12,   180] loss: 0.323\n",
      "Time: 3.5485188961029053\n",
      "[13,    20] loss: 0.278\n",
      "Time: 3.589158296585083\n",
      "[13,    40] loss: 0.299\n",
      "Time: 3.480269193649292\n",
      "[13,    60] loss: 0.234\n",
      "Time: 3.5599782466888428\n",
      "[13,    80] loss: 0.368\n",
      "Time: 3.5500500202178955\n",
      "[13,   100] loss: 0.335\n",
      "Time: 3.380739688873291\n",
      "[13,   120] loss: 0.324\n",
      "Time: 3.569117307662964\n",
      "[13,   140] loss: 0.359\n",
      "Time: 3.470242738723755\n",
      "[13,   160] loss: 0.310\n",
      "Time: 3.5230791568756104\n",
      "[13,   180] loss: 0.284\n",
      "Time: 3.426670551300049\n",
      "[14,    20] loss: 0.342\n",
      "Time: 3.495481014251709\n",
      "[14,    40] loss: 0.259\n",
      "Time: 3.5258052349090576\n",
      "[14,    60] loss: 0.304\n",
      "Time: 3.4904966354370117\n",
      "[14,    80] loss: 0.321\n",
      "Time: 3.583003520965576\n",
      "[14,   100] loss: 0.215\n",
      "Time: 3.58414626121521\n",
      "[14,   120] loss: 0.351\n",
      "Time: 3.5530338287353516\n",
      "[14,   140] loss: 0.293\n",
      "Time: 3.516669511795044\n",
      "[14,   160] loss: 0.248\n",
      "Time: 3.5447630882263184\n",
      "[14,   180] loss: 0.323\n",
      "Time: 3.557190418243408\n",
      "[15,    20] loss: 0.351\n",
      "Time: 3.5232975482940674\n",
      "[15,    40] loss: 0.328\n",
      "Time: 3.4045839309692383\n",
      "[15,    60] loss: 0.223\n",
      "Time: 3.4155001640319824\n",
      "[15,    80] loss: 0.216\n",
      "Time: 3.53857684135437\n",
      "[15,   100] loss: 0.278\n",
      "Time: 3.4962000846862793\n",
      "[15,   120] loss: 0.330\n",
      "Time: 3.5370352268218994\n",
      "[15,   140] loss: 0.378\n",
      "Time: 3.493659734725952\n",
      "[15,   160] loss: 0.334\n",
      "Time: 3.4722821712493896\n",
      "[15,   180] loss: 0.323\n",
      "Time: 3.5224297046661377\n",
      "[16,    20] loss: 0.228\n",
      "Time: 3.4837076663970947\n",
      "[16,    40] loss: 0.259\n",
      "Time: 3.574381113052368\n",
      "[16,    60] loss: 0.317\n",
      "Time: 3.6048150062561035\n",
      "[16,    80] loss: 0.394\n",
      "Time: 3.5521953105926514\n",
      "[16,   100] loss: 0.311\n",
      "Time: 3.534377098083496\n",
      "[16,   120] loss: 0.275\n",
      "Time: 3.559011220932007\n",
      "[16,   140] loss: 0.294\n",
      "Time: 3.413538932800293\n",
      "[16,   160] loss: 0.282\n",
      "Time: 3.438002824783325\n",
      "[16,   180] loss: 0.297\n",
      "Time: 3.480161666870117\n",
      "[17,    20] loss: 0.204\n",
      "Time: 3.429368734359741\n",
      "[17,    40] loss: 0.206\n",
      "Time: 3.410238742828369\n",
      "[17,    60] loss: 0.254\n",
      "Time: 3.439861536026001\n",
      "[17,    80] loss: 0.240\n",
      "Time: 3.3648085594177246\n",
      "[17,   100] loss: 0.213\n",
      "Time: 3.4302523136138916\n",
      "[17,   120] loss: 0.224\n",
      "Time: 3.543297529220581\n",
      "[17,   140] loss: 0.342\n",
      "Time: 3.495351552963257\n",
      "[17,   160] loss: 0.234\n",
      "Time: 3.5599124431610107\n",
      "[17,   180] loss: 0.255\n",
      "Time: 3.5463898181915283\n",
      "[18,    20] loss: 0.260\n",
      "Time: 3.537496328353882\n",
      "[18,    40] loss: 0.260\n",
      "Time: 3.6250534057617188\n",
      "[18,    60] loss: 0.227\n",
      "Time: 3.50480055809021\n",
      "[18,    80] loss: 0.207\n",
      "Time: 3.4970781803131104\n",
      "[18,   100] loss: 0.220\n",
      "Time: 3.5803489685058594\n",
      "[18,   120] loss: 0.210\n",
      "Time: 3.66308856010437\n",
      "[18,   140] loss: 0.291\n",
      "Time: 2.996328353881836\n",
      "[18,   160] loss: 0.185\n",
      "Time: 2.6927170753479004\n",
      "[18,   180] loss: 0.209\n",
      "Time: 2.6889126300811768\n",
      "[19,    20] loss: 0.295\n",
      "Time: 2.658879041671753\n",
      "[19,    40] loss: 0.298\n",
      "Time: 3.4431426525115967\n",
      "[19,    60] loss: 0.264\n",
      "Time: 3.520533800125122\n",
      "[19,    80] loss: 0.209\n",
      "Time: 3.5595297813415527\n",
      "[19,   100] loss: 0.245\n",
      "Time: 3.483952045440674\n",
      "[19,   120] loss: 0.251\n",
      "Time: 3.402074098587036\n",
      "[19,   140] loss: 0.259\n",
      "Time: 3.5148236751556396\n",
      "[19,   160] loss: 0.241\n",
      "Time: 3.125497817993164\n",
      "[19,   180] loss: 0.236\n",
      "Time: 2.7563066482543945\n",
      "[20,    20] loss: 0.157\n",
      "Time: 2.7216060161590576\n",
      "[20,    40] loss: 0.199\n",
      "Time: 2.8354053497314453\n",
      "[20,    60] loss: 0.198\n",
      "Time: 2.791600227355957\n",
      "[20,    80] loss: 0.251\n",
      "Time: 2.69911527633667\n",
      "[20,   100] loss: 0.190\n",
      "Time: 2.748056411743164\n",
      "[20,   120] loss: 0.231\n",
      "Time: 2.823601484298706\n",
      "[20,   140] loss: 0.151\n",
      "Time: 3.0785739421844482\n",
      "[20,   160] loss: 0.182\n",
      "Time: 3.542346477508545\n",
      "[20,   180] loss: 0.255\n",
      "Time: 3.727360725402832\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 88 %\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "def train():\n",
    "    # TRANSFORMATION AUGMENTATION\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        # Turn the image into a torch.Tensor\n",
    "        transforms.ToTensor(),  # converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "        # ConvFiltersTransform(axis=(1, 2)),\n",
    "        # transforms.Resize(size=(224, 224))\n",
    "    ])\n",
    "\n",
    "    # DATA SETS\n",
    "    train_data = datasets.ImageFolder(root='./dataset/train', transform=data_transform)\n",
    "    test_data = datasets.ImageFolder(root='./dataset/test', transform=data_transform)\n",
    "\n",
    "    # DATA LOADER\n",
    "    train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "    # print(train_data[0][0].shape)  # C, H, W\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=3)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            # Convolutional blocks\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "            # self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "            # self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "            self.conv4_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "            self.conv4_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "            # Left side\n",
    "            self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "            self.conv6 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "            self.conv7 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "            # Right side\n",
    "            self.conv8 = nn.Conv2d(192, 256, kernel_size=3, padding=1)\n",
    "            self.conv9 = nn.Conv2d(192, 256, kernel_size=3, padding=1)\n",
    "\n",
    "            #\n",
    "            self.conv10 = nn.Conv2d(512, 920, kernel_size=3, padding=1)\n",
    "\n",
    "            # Pooling layers\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=5, stride=5, padding=0)\n",
    "            self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=4, padding=0)\n",
    "            self.avgpool2 = nn.AdaptiveAvgPool2d((3, 3))\n",
    "\n",
    "            # Fully connected layers (for now, we'll keep this as a placeholder)\n",
    "            self.fc1 = nn.Linear(920 * 3 * 3, 920)  # Adjust this later dynamically\n",
    "            # self.fc2 = nn.Linear(920, 256)\n",
    "            # self.fc3 = nn.Linear(256, 128)\n",
    "            # self.fc4 = nn.Linear(128, 64)\n",
    "            # self.fc5 = nn.Linear(64, 32)\n",
    "            self.fc6 = nn.Linear(920, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "            # Pooling\n",
    "            x = self.maxpool(x)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.relu(self.conv3(x))\n",
    "            # Left side\n",
    "            x_1 = F.relu(self.conv4_1(x))\n",
    "            x_L = F.relu(self.conv5(x_1))\n",
    "            x_L_1 = F.relu(self.conv6(x_L))\n",
    "            x_L_2 = F.relu(self.conv7(x_L))\n",
    "            x_L = F.relu(torch.add(x_L_1, x_L_2))\n",
    "            # Right side\n",
    "            x_2 = F.relu(self.conv4_2(x))\n",
    "            x_R = torch.cat((x_1, x_2), 1)\n",
    "            x_R_1 = F.relu(self.conv8(x_R))\n",
    "            x_R_2 = F.relu(self.conv9(x_R))\n",
    "            x_R = F.relu(torch.add(x_R_1, x_R_2))\n",
    "            #\n",
    "            x = torch.cat((x_L, x_R), 1)\n",
    "            x = F.relu(self.avgpool1(x))\n",
    "            x = F.relu(self.conv10(x))\n",
    "            x = F.relu(self.avgpool2(x))\n",
    "            #\n",
    "            x = x.view(x.size(0), -1)  # Flatten\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # x = self.fc2(x)\n",
    "            # x = self.fc3(x)\n",
    "            # x = self.fc4(x)\n",
    "            # x = self.fc5(x)\n",
    "            x = self.fc6(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on: {device}\")\n",
    "\n",
    "    net.to(device)\n",
    "    summary(net, input_size=(3, 224, 224))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        time_start = time.time()\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "                print('Time:', time.time() - time_start)\n",
    "                time_start = time.time()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
